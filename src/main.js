import archiver from 'archiver';
import { Client, Storage, Databases, Query, Permission, Role } from 'node-appwrite';

export default async function prepareDownload(context) {
  context.log('üîπ Starting download preparation function...');

  let payload = {};
  try {
    context.log('üì• Parsing request body...');
    if (context.req.bodyRaw) {
      payload = JSON.parse(context.req.bodyRaw);
      context.log('‚úÖ Request body parsed:', payload);
    }
  } catch (err) {
    context.error('‚ùå Invalid JSON in request body: ' + err.message);
    return context.res.json({ statusCode: 400, error: 'Invalid JSON in request body' });
  }

  const { eventId } = payload;
  
  if (!eventId) {
    context.error('‚ùå Missing eventId in request body');
    return context.res.json({ statusCode: 400, error: 'Missing eventId' });
  }

  context.log(`üìå Event ID: ${eventId}`);

  // Initialize Appwrite client
  const client = new Client()
    .setEndpoint(process.env.APPWRITE_ENDPOINT)
    .setProject(process.env.APPWRITE_PROJECT_ID)
    .setKey(process.env.APPWRITE_API_KEY);
  context.log('üîó Appwrite client initialized');

  const storage = new Storage(client);
  const databases = new Databases(client);

  const databaseId = process.env.APPWRITE_DATABASE_ID;
  const photoCollectionId = process.env.APPWRITE_PHOTO_COLLECTION_ID;
  const eventCollectionId = process.env.APPWRITE_EVENT_COLLECTION_ID;
  const downloadCollectionId = process.env.APPWRITE_DOWNLOAD_COLLECTION_ID;
  const photoBucketId = process.env.APPWRITE_BUCKET_ID;
  const downloadBucketId = process.env.APPWRITE_DOWNLOAD_BUCKET_ID;

  // Verify user ownership
  const headers = context.req.headers;
  const currentUserId = headers['x-appwrite-user-id'];
  context.log(`üë§ Current user ID from headers: ${currentUserId}`);

  try {
    // 1Ô∏è‚É£ Verify event ownership
    context.log(`üîπ Verifying event ownership for ${eventId}...`);
    const eventDoc = await databases.getDocument(databaseId, eventCollectionId, eventId);
    
    const eventUserId = String(eventDoc.user_id || '').trim();
    context.log(`üîë Event owner: ${eventUserId}`);
    
    if (eventUserId !== currentUserId) {
      context.error(`‚ùå Ownership mismatch: event.user_id=${eventUserId}, user=${currentUserId}`);
      return context.res.json({
        statusCode: 403,
        error: 'Forbidden ‚Äì you do not own this event',
      });
    }
    context.log(`üîí Ownership verified for user ${currentUserId}`);

    // 2Ô∏è‚É£ Fetch ALL photos with their sizes
    context.log(`üîπ Fetching all photos for event ${eventId}...`);
    const allPhotos = [];
    let offset = 0;
    const batchSize = 100;

    while (true) {
      const result = await databases.listDocuments(databaseId, photoCollectionId, [
        Query.equal('event_id', eventId),
        Query.limit(batchSize),
        Query.offset(offset),
      ]);

      if (result.documents.length === 0) break;
      
      allPhotos.push(...result.documents);
      context.log(`üìÑ Fetched ${result.documents.length} photos (total: ${allPhotos.length})`);
      
      offset += result.documents.length;
      if (result.documents.length < batchSize) break;
    }

    if (allPhotos.length === 0) {
      context.log('‚ö†Ô∏è No photos found');
      return context.res.json({ statusCode: 404, error: 'No photos found' });
    }

    context.log(`‚úÖ Total photos to process: ${allPhotos.length}`);

    // 3Ô∏è‚É£ Split photos into 2GB chunks
    const MAX_CHUNK_SIZE_MB = 2048; // 2GB in MB
    const chunks = [];
    let currentChunk = [];
    let currentChunkSize = 0;

    for (const photo of allPhotos) {
      const photoSize = parseFloat(photo.file_size || 0);
      
      if (currentChunkSize + photoSize > MAX_CHUNK_SIZE_MB && currentChunk.length > 0) {
        // Current chunk would exceed 2GB, start new chunk
        chunks.push([...currentChunk]);
        currentChunk = [];
        currentChunkSize = 0;
      }
      
      currentChunk.push(photo);
      currentChunkSize += photoSize;
    }

    // Add remaining photos
    if (currentChunk.length > 0) {
      chunks.push(currentChunk);
    }

    context.log(`üì¶ Created ${chunks.length} chunks (max 2GB each)`);

    // 4Ô∏è‚É£ Process each chunk
    const createdDownloads = [];

    for (let chunkIndex = 0; chunkIndex < chunks.length; chunkIndex++) {
      const chunk = chunks[chunkIndex];
      const zipFilename = chunks.length > 1 
        ? `${eventDoc.event_name || 'photos'}_part_${chunkIndex + 1}.zip`
        : `${eventDoc.event_name || 'photos'}.zip`;

      context.log(`üîπ Processing chunk ${chunkIndex + 1}/${chunks.length} (${chunk.length} photos)`);

      // Create ZIP archive
      const archive = archiver('zip', { zlib: { level: 0 } });
      const zipChunks = [];

      archive.on('data', (data) => zipChunks.push(data));
      
      let archiveFinished = false;
      archive.on('end', () => { archiveFinished = true; });
      archive.on('error', (err) => { throw err; });

      // Add photos to archive
      for (const [index, photo] of chunk.entries()) {
        try {
          const fileId = photo.file_id;
          const fileType = photo.file_type || 'jpg';
          const photoId = photo.$id;
          const filename = `photo_${photoId}.${fileType}`;
          
          context.log(`üì∏ [${index + 1}/${chunk.length}] Adding: ${filename}`);

          const fileData = await storage.getFileDownload(photoBucketId, fileId);
          const fileBuffer = Buffer.isBuffer(fileData) ? fileData : Buffer.from(fileData);
          
          archive.append(fileBuffer, { name: filename });
        } catch (err) {
          context.error(`‚ùå Failed to add photo ${photo.$id}: ${err.message}`);
        }
      }

      // Finalize archive
      await archive.finalize();
      
      // Wait for archive to finish
      while (!archiveFinished) {
        await new Promise(resolve => setTimeout(resolve, 100));
      }

      const zipBuffer = Buffer.concat(zipChunks);
      const zipSizeMB = (zipBuffer.length / 1024 / 1024).toFixed(2);
      context.log(`‚úÖ ZIP created: ${zipSizeMB} MB`);

      // 5Ô∏è‚É£ Upload to download bucket
      const downloadFileId = `download_${Date.now()}_${chunkIndex}`;
      context.log(`üì§ Uploading to download bucket: ${downloadFileId}`);

      const { Readable } = require('stream');
      const readable = new Readable();
      readable.push(zipBuffer);
      readable.push(null);

      const uploadedFile = await storage.createFile(
        downloadBucketId,
        downloadFileId,
        readable,
        [
          Permission.read(Role.user(currentUserId)),
          Permission.delete(Role.user(currentUserId))
        ]
      );

      context.log(`‚úÖ Uploaded: ${uploadedFile.$id}`);

      // 6Ô∏è‚É£ Create download record
      const downloadDoc = await databases.createDocument(
        databaseId,
        downloadCollectionId,
        'unique()',
        {
          user_id: currentUserId,
          event_id: eventId,
          file_id: uploadedFile.$id,
          file_name: zipFilename,
          size_mb: parseFloat(zipSizeMB),
          photo_count: chunk.length,
          chunk_index: chunkIndex + 1,
          total_chunks: chunks.length,
        }
      );

      context.log(`üìù Download record created: ${downloadDoc.$id}`);
      createdDownloads.push(downloadDoc.$id);
    }

    context.log(`‚úÖ All chunks processed successfully`);
    
    return context.res.json({
      statusCode: 200,
      message: 'Download prepared successfully',
      totalChunks: chunks.length,
      downloadIds: createdDownloads,
    });

  } catch (error) {
    context.error('‚ùå Error preparing download: ' + error.message);
    context.error('Stack trace:', error.stack);
    return context.res.json({ statusCode: 500, error: error.message });
  }
}